``` I I have done this * Use a subset of the UCF101 dataset, focusing on actions with consistent motion (e.g., "Walking," "Jumping," or "Biking"). You should select at least 5 classes. * Preprocess video frames by resizing them to 64x64 pixels and converting them to grayscale or RGB to manage computational requirements. And stored the relevant frames in processed_data with train and test directories of each category  Print as much as you can on the console so I can debug if needed, save the trained model as well  * Train the model to take a short input sequence (e.g., 10 frames) and predict the next several frames (e.g., 5-10 frames) to simulate continuous motion. * Show the results for these as these processes happen as well  Keep in mind the (from scratch phrase) Model Selection: Implement (from scratch): PredRNN for advanced temporal modeling. Use this research paper summary to help you with the implementation ``` Summary of the Research Paper: PredRNN - Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs **Authors**: Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S. Yu **Conference**: NIPS 2017 **Overview** The paper introduces PredRNN, a recurrent neural network framework for spatiotemporal predictive learning tasks, such as video frame prediction. The model addresses the challenge of generating future frames by effectively memorizing spatial appearances and temporal dynamics through a unified memory mechanism. **Key Contributions** 1. **Unified Spatiotemporal Memory**:    * Conventional LSTM-based methods focus mainly on temporal dynamics, leading to loss of spatial details. PredRNN introduces a unified memory pool to handle both spatial and temporal information.    * Memory states flow in a zigzag pattern—vertically across stacked layers and horizontally over time steps. 2. **Spatiotemporal LSTM (ST-LSTM)**:    * A novel LSTM unit that maintains two types of memory:       * Standard temporal memory for capturing temporal dependencies.       * Spatiotemporal memory for modeling spatial correlations and interactions across layers.    * A gate-controlled dual memory mechanism facilitates efficient information flow and fusion. 3. **Improved Predictive Performance**:    * Outperforms state-of-the-art models on several benchmarks, including synthetic and real-world datasets, with sharper predictions and better long-term accuracy. **Experiments and Results** 1. **Datasets Used**:    * Moving MNIST: Predicts the motion of digits in a 64×64 grid.    * KTH Action Dataset: Predicts human actions in video sequences.    * Radar Echo Dataset: Models precipitation patterns from radar images. 2. **Metrics**:    * Evaluated using Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM).    * PredRNN consistently achieves lower MSE and higher PSNR/SSIM compared to baselines like ConvLSTM, VPN, and CDNA. 3. **Qualitative Observations**:    * PredRNN generates visually sharp and accurate long-term predictions, preserving both motion trajectories and object details better than competing models. 4. **Efficiency**:    * Requires significantly less computational memory and training time than advanced models like Video Pixel Networks (VPN). **Conclusion** PredRNN introduces a novel architecture with Spatiotemporal LSTM units to jointly model spatial and temporal structures, achieving state-of-the-art performance in predictive learning tasks. Its modular design also allows easy extension to other applications beyond video prediction, such as weather forecasting and traffic prediction.